{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa0258e-3ee8-49cf-af47-9691308b55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28851f5e-a9d1-476a-b772-b285589c7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import os"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cc07a65-1fec-4694-8e0f-7b15e78e2d3b",
   "metadata": {},
   "source": [
    "# Define data transformations for data augmentation and normalization\n",
    "    # CASO A IMAGEM FOSSE DO TIPO RGB\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20bfa0c2-7d40-403a-a0f1-d3e7d674cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31839e0e-1590-4ff8-923d-8af5a0c5bb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 112.30059990516075\n",
      "Standard Deviation: 85.98406628052774\n"
     ]
    }
   ],
   "source": [
    "# Calculating the mean and standard deviation of the data set\n",
    "# dataset_path = 'C:\\\\Users\\\\Khomp\\\\Desktop\\\\Projetos SBC 2025\\\\ResNet18\\\\extra\\\\dataset\\\\test\\\\healthy'\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def load_images_from_folder(folder, target_size=(256, 256)):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, target_size)\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "def compute_mean_std(images):\n",
    "    stacked_images = np.stack(images, axis=0)\n",
    "    mean = np.mean(stacked_images)\n",
    "    std = np.std(stacked_images)\n",
    "    return mean, std\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\Khomp\\\\Desktop\\\\Projetos SBC 2025\\\\ResNet18\\\\extra\\\\checandoMEDIAeSD'\n",
    "images = load_images_from_folder(folder_path)\n",
    "mean, std = compute_mean_std(images)\n",
    "\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Standard Deviation: {std}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c175733-d7d7-4c89-b250-aafab11e9065",
   "metadata": {},
   "source": [
    "# USEI ISSO DAQUI QUANDO NÃO SABIA A MÉDIA e o STD das imagens do meu dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define data transformations for grayscale images\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])  # Normalize using mean and std for grayscale\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])  # Normalize using mean and std for grayscale\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "def24fe0-a702-4c72-9134-943db25be5ca",
   "metadata": {},
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# Open the image file\n",
    "# image_path = 'C:\\\\Users\\\\Khomp\\\\Desktop\\\\Projetos SBC 2025\\\\ResNet18\\\\extra\\\\dataset\\\\val\\\\healthy\\\\MCUCXR_0068_0.png'\n",
    "##  image_path = r'C:\\Users\\Khomp\\Desktop\\Projetos SBC 2025\\ResNet18\\extra\\dataset\\val\\healthy\\MCUCXR_0068_0.png'\n",
    "\n",
    "# image = Image.open(image_path)\n",
    "\n",
    "# Check the mode of the image\n",
    "# if image.mode == 'RGB':\n",
    "#     print('The image is in RGB mode.')\n",
    "# else:\n",
    "#     print(f'The image is not in RGB mode. Its mode is: {image.mode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def97318-a3eb-4519-bb83-d54982af2594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations for grayscale images with the computed mean and std\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([112.3006 / 255], [85.9841 / 255])  # Normalize using the computed mean and std\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([112.3006 / 255], [85.9841 / 255])  # Normalize using the computed mean and std\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc77b86-69eb-42ff-a1a6-d6ed916e7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory\n",
    "data_dir = 'dataset'\n",
    "\n",
    "# Create data loaders\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "#image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d0d8e0-e33b-4871-90ca-a503ea2eae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 91, 'val': 13}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['healthy', 'tuberculosis']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "print(dataset_sizes)\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "860468fd-4e9e-4618-9172-0b8ef2354c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khomp\\USPesalqMBA\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Khomp\\USPesalqMBA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained ResNet-18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final classification layer\n",
    "for name, param in model.named_parameters():\n",
    "    if \"fc\" in name:  # Unfreeze the final classification layer\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # Use all parameters\n",
    "\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dece257-d309-4b69-aeb8-44fcd6bbf3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.2062 Acc: 0.3407\n",
      "val Loss: 0.9148 Acc: 0.5385\n",
      "train Loss: 0.9394 Acc: 0.5495\n",
      "val Loss: 0.9567 Acc: 0.3846\n",
      "train Loss: 0.8231 Acc: 0.6044\n",
      "val Loss: 0.6294 Acc: 0.7692\n",
      "train Loss: 0.8442 Acc: 0.5604\n",
      "val Loss: 0.5531 Acc: 0.7692\n",
      "train Loss: 0.9432 Acc: 0.5275\n",
      "val Loss: 1.0009 Acc: 0.6154\n",
      "train Loss: 0.7401 Acc: 0.5824\n",
      "val Loss: 0.6434 Acc: 0.6154\n",
      "train Loss: 0.6441 Acc: 0.6374\n",
      "val Loss: 0.7308 Acc: 0.6923\n",
      "train Loss: 0.8055 Acc: 0.5714\n",
      "val Loss: 0.5981 Acc: 0.6923\n",
      "train Loss: 0.8067 Acc: 0.5934\n",
      "val Loss: 0.9022 Acc: 0.6923\n",
      "train Loss: 0.7374 Acc: 0.6264\n",
      "val Loss: 0.5514 Acc: 0.6923\n",
      "train Loss: 0.7178 Acc: 0.6484\n",
      "val Loss: 0.8352 Acc: 0.6923\n",
      "train Loss: 0.6843 Acc: 0.6484\n",
      "val Loss: 0.7086 Acc: 0.5385\n",
      "train Loss: 0.5983 Acc: 0.7143\n",
      "val Loss: 1.0041 Acc: 0.6923\n",
      "train Loss: 0.5834 Acc: 0.7253\n",
      "val Loss: 0.7919 Acc: 0.6154\n",
      "train Loss: 0.7738 Acc: 0.6593\n",
      "val Loss: 0.7417 Acc: 0.5385\n",
      "train Loss: 0.7584 Acc: 0.6484\n",
      "val Loss: 0.8931 Acc: 0.6923\n",
      "train Loss: 0.7450 Acc: 0.6484\n",
      "val Loss: 1.0793 Acc: 0.6923\n",
      "train Loss: 0.6973 Acc: 0.6703\n",
      "val Loss: 0.8476 Acc: 0.6154\n",
      "train Loss: 0.8513 Acc: 0.6593\n",
      "val Loss: 1.0664 Acc: 0.6154\n",
      "train Loss: 0.5918 Acc: 0.6593\n",
      "val Loss: 0.8921 Acc: 0.6154\n",
      "train Loss: 0.5566 Acc: 0.7033\n",
      "val Loss: 0.7929 Acc: 0.6154\n",
      "train Loss: 0.8450 Acc: 0.6484\n",
      "val Loss: 1.3641 Acc: 0.6923\n",
      "train Loss: 0.6145 Acc: 0.7363\n",
      "val Loss: 0.8228 Acc: 0.6154\n",
      "train Loss: 0.6093 Acc: 0.6923\n",
      "val Loss: 0.9470 Acc: 0.6154\n",
      "train Loss: 0.5244 Acc: 0.7143\n",
      "val Loss: 0.6798 Acc: 0.6154\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Convert grayscale images to RGB\n",
    "            if inputs.shape[1] == 1:\n",
    "                inputs = inputs.repeat(1, 3, 1, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "594a765c-82f6-4bc8-861a-292819477adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy: 0.7912\n",
      "train Precision: 0.7911\n",
      "train Recall: 0.7912 \n",
      "\n",
      "val Accuracy: 0.6154\n",
      "val Precision: 0.6026\n",
      "val Recall: 0.6154 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================== Evaluation =========================\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, dataloaders, dataset_sizes, device):\n",
    "    model.eval()\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Convert grayscale images to RGB\n",
    "            if inputs.shape[1] == 1:\n",
    "                inputs = inputs.repeat(1, 3, 1, 1)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Convert lists to NumPy arrays\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        print(f'{phase} Accuracy: {accuracy:.4f}')\n",
    "        print(f'{phase} Precision: {precision:.4f}')\n",
    "        print(f'{phase} Recall: {recall:.4f} \\n')\n",
    "        \n",
    "\n",
    "# Call the function after training the model\n",
    "evaluate_model(model, dataloaders, dataset_sizes, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9361eaa-2a46-4863-a65d-b3278888f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'chest_xray_classification_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fcbba6-4180-45ac-905d-ab89117b77ea",
   "metadata": {},
   "source": [
    "Classification on Unseen Image\n",
    "To use the saved model to classify unseen images, we need to load the model and then apply it to the new images for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f20f675-ffb6-4dbc-9f3c-ae79dc634b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khomp\\USPesalqMBA\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Khomp\\USPesalqMBA\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the saved model\n",
    "model = models.resnet18(pretrained=True)\n",
    "#model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1000)  # Adjust to match the original model's output units\n",
    "model.load_state_dict(torch.load('chest_xray_classification_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a new model with the correct final layer\n",
    "new_model = models.resnet18(pretrained=True)\n",
    "new_model.fc = nn.Linear(new_model.fc.in_features, 2)  # Adjust to match the desired output units\n",
    "\n",
    "# Copy the weights and biases from the loaded model to the new model\n",
    "new_model.fc.weight.data = model.fc.weight.data[0:2]  # Copy only the first 2 output units\n",
    "new_model.fc.bias.data = model.fc.bias.data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bdb1ac-d817-4ac2-aef6-b50193a99491",
   "metadata": {},
   "source": [
    "Preparing new image for classification. You should use the same data transformations you used during training. Here's an example of how to prepare an image for inference:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40c0c466-142f-4714-8537-2b860ec5c080",
   "metadata": {},
   "source": [
    "# Load and preprocess the unseen image\n",
    "image_path = 'MCUCXR_0084_0.png'  # Replace with the path to the image\n",
    "image = Image.open(image_path)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    transforms.Normalize([112.3006 / 255], [85.9841 / 255])  # Normalize using the computed mean and std\n",
    "])\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e62ff5c-c1f2-4f06-8213-f8eaf895ac11",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# Load and preprocess the unseen image\n",
    "image_path = 'MCUCXR_0372_1.png'  # Replace with the path to the image\n",
    "# image_path = 'MCUCXR_0084_0.png'  # Replace with the path to the image\n",
    "image = Image.open(image_path).convert('RGB')  # Convert image to RGB\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Check if CUDA is available and use it if possible\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "\n",
    "# Display tensor shape to confirm it is correct\n",
    "print(input_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6e9d15-2d4e-4f55-8b1c-c939b0f59ca4",
   "metadata": {},
   "source": [
    "Performing inference using the model:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7eea151-8564-4768-9475-c8ce564d98d3",
   "metadata": {},
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Get the predicted class\n",
    "_, predicted_class = output.max(1)\n",
    "\n",
    "# Map the predicted class to the class name\n",
    "class_names = ['healthy', 'tuberculosis']  # Make sure these class names match the training data\n",
    "predicted_class_name = class_names[predicted_class.item()]\n",
    "\n",
    "print(f'The predicted class is: {predicted_class_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7c081c1-a7f4-4f38-99e9-e2772824f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe6572e7-862e-426e-864e-05eb18177dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_image(image):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    input_tensor = preprocess(image)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "    return input_batch\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(image):\n",
    "    input_batch = preprocess_image(image)\n",
    "\n",
    "    # Check if CUDA is available and use it if possible\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        new_model.to('cuda')\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = new_model(input_batch)\n",
    "\n",
    "    # Get the predicted class\n",
    "    _, predicted_class = output.max(1)\n",
    "    class_names = ['healthy', 'tuberculosis']  # Make sure these class names match the training data\n",
    "    predicted_class_name = class_names[predicted_class.item()]\n",
    "\n",
    "    return predicted_class_name, image\n",
    "\n",
    "# Create the Gradio interface\n",
    "def gradio_predict(image):\n",
    "    predicted_class_name, image = predict(image)\n",
    "    displayed_image = display_image_with_prediction(predicted_class_name, image)\n",
    "    return predicted_class_name, displayed_image\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_predict,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=[\"text\", \"image\"],\n",
    "    title=\"Chest X-ray Classification\",\n",
    "    description=\"Upload a chest X-ray image to classify it as healthy or tuberculosis.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()\n",
    "\n",
    "# Define a function to display the image with the predicted class name\n",
    "def display_image_with_prediction(predicted_class_name, image):\n",
    "    # Display the image with the predicted class name\n",
    "    image_np = np.array(image)\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')\n",
    "    plt.text(10, 10, f'Predicted: {predicted_class_name}', fontsize=12, color='white', backgroundcolor='red')\n",
    "    plt.savefig(\"predicted_image.png\")  # Save the plot as an image file\n",
    "    return Image.open(\"predicted_image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9c375-7601-4ce8-865c-6135c7ee545a",
   "metadata": {},
   "source": [
    "DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7cb52-1bc0-4092-8292-99f86d311402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
